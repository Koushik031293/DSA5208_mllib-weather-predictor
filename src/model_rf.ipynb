{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JAVA_HOME = /Library/Java/JavaVirtualMachines/temurin-11.jdk/Contents/Home\n",
      "java -version:\n",
      "openjdk version \"11.0.28\" 2025-07-15\n",
      "OpenJDK Runtime Environment Temurin-11.0.28+6 (build 11.0.28+6)\n",
      "OpenJDK 64-Bit Server VM Temurin-11.0.28+6 (build 11.0.28+6, mixed mode)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/10/16 12:40:46 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# --- First cell: force Java 11 inside the notebook process ---\n",
    "import os, subprocess, sys\n",
    "\n",
    "# Point JAVA_HOME to JDK 11 (Temurin 11) on macOS\n",
    "os.environ[\"JAVA_HOME\"] = subprocess.check_output(\n",
    "    [\"/usr/libexec/java_home\", \"-v\", \"11\"], text=True\n",
    ").strip()\n",
    "\n",
    "# sanity check\n",
    "print(\"JAVA_HOME =\", os.environ[\"JAVA_HOME\"])\n",
    "print(\"java -version:\")\n",
    "print(subprocess.check_output([\"java\", \"-version\"], stderr=subprocess.STDOUT, text=True))\n",
    "\n",
    "# EDA: Setup & Load\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"ISD_2024_RF\")\n",
    "      .master(\"local[*]\")  # local run\n",
    "      .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n",
    "      .config(\"spark.sql.shuffle.partitions\", \"500\")   # up from 200 for 18M rows\n",
    "      .config(\"spark.default.parallelism\", \"500\")\n",
    "      .config(\"spark.driver.memory\", \"16g\")             # if you have the RAM; else keep 8g\n",
    "      .config(\"spark.memory.fraction\", \"0.6\")\n",
    "      .config(\"spark.memory.storageFraction\", \"0.4\")\n",
    "      .config(\"spark.kryoserializer.buffer.max\", \"1024m\")\n",
    "      .getOrCreate()\n",
    ")\n",
    "# PARQUET_PATH = \"../data/cleansed/2024_cleansed.parquet\"  \n",
    "# df = spark.read.parquet(PARQUET_PATH)\n",
    "\n",
    "# print(\"Loaded:\", PARQUET_PATH)\n",
    "# df.printSchema()\n",
    "# print(\"Rows:\", f\"{df.count():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "train_output = \"../data/train_2024.parquet\"\n",
    "test_output  = \"../data/test_2024.parquet\"\n",
    "\n",
    "train_df = spark.read.parquet(train_output)\n",
    "test_df = spark.read.parquet(test_output)\n",
    "\n",
    "# print(\"Loaded:\", train_df)\n",
    "# train_df.printSchema()\n",
    "# print(\"Rows:\", f\"{train_df.count():,}\")\n",
    "\n",
    "# print(\"Loaded:\", test_df)\n",
    "# test_df.printSchema()\n",
    "# print(\"Rows:\", f\"{test_df.count():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/15 21:44:00 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "25/10/15 21:49:46 WARN DAGScheduler: Broadcasting large task binary with size 1676.2 KiB\n",
      "25/10/15 21:52:05 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "25/10/15 21:54:58 WARN DAGScheduler: Broadcasting large task binary with size 6.4 MiB\n",
      "25/10/15 21:58:30 WARN DAGScheduler: Broadcasting large task binary with size 1986.1 KiB\n",
      "25/10/15 21:58:51 WARN DAGScheduler: Broadcasting large task binary with size 12.6 MiB\n",
      "25/10/15 22:03:58 WARN DAGScheduler: Broadcasting large task binary with size 3.9 MiB\n",
      "WARNING: An illegal reflective access operation has occurred                    \n",
      "WARNING: Illegal reflective access by org.apache.spark.util.SizeEstimator$ (file:/Users/koushik/Desktop/Machine%20Learning/.conda/lib/python3.12/site-packages/pyspark/jars/spark-core_2.12-3.5.1.jar) to field java.nio.charset.Charset.name\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.util.SizeEstimator$\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "[Stage 77:=====================================================>  (18 + 1) / 19]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RF-local] RMSE=3.7224 | MAE=2.6887 | R²=0.9123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.feature import Imputer, VectorAssembler\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark import StorageLevel\n",
    "# Serialized MEMORY_AND_DISK equivalent\n",
    "StorageLevel(useDisk=True, useMemory=True, useOffHeap=False, deserialized=False, replication=1)\n",
    "\n",
    "label_col = \"TMP_C\"\n",
    "feature_cols = [\n",
    "    \"LATITUDE\",\"LONGITUDE\",\"ELEVATION\",\n",
    "    \"WND_DIR_DEG\",\"WND_SPD_MS\",\n",
    "    \"CIG_HEIGHT_M\",\"VIS_DIST_M\",\n",
    "    \"DEW_C\",\"SLP_hPa\",\n",
    "    \"year\",\"month\",\"day\",\"hour\"\n",
    "]\n",
    "\n",
    "# Keep DATE only in test for reporting\n",
    "train_df = train_df.select(label_col, *feature_cols).repartition(500)\n",
    "test_df  = test_df.select(\"DATE\", label_col, *feature_cols)\n",
    "\n",
    "# --- Fit lightweight stages ONCE (minimize broadcast size) ---\n",
    "imp_cols = [c + \"_imp\" for c in feature_cols]\n",
    "imputer = Imputer(strategy=\"median\", inputCols=feature_cols, outputCols=imp_cols)\n",
    "imputerModel = imputer.fit(train_df)\n",
    "\n",
    "assembler = VectorAssembler(inputCols=imp_cols, outputCol=\"features\")\n",
    "\n",
    "# Transform & cache the vectorized train once (serialized)\n",
    "train_feat = assembler.transform(imputerModel.transform(train_df)) \\\n",
    "                       .select(label_col, \"features\") \\\n",
    "                       .persist(StorageLevel(True, True, False, False, 1))\n",
    "_ = train_feat.count()  # materialize to break lineage\n",
    "\n",
    "test_feat = assembler.transform(imputerModel.transform(test_df)) \\\n",
    "                      .select(\"DATE\", label_col, \"features\")\n",
    "\n",
    "# --- Random Forest: start lean; scale up if metrics improve materially ---\n",
    "rf = RandomForestRegressor(\n",
    "    featuresCol=\"features\", labelCol=label_col,\n",
    "    numTrees=80,          # try 60–100; increase only if you see clear gains\n",
    "    maxDepth=10,          # 8–10 is a sweet spot on laptop\n",
    "    subsamplingRate=0.7,\n",
    "    featureSubsetStrategy=\"sqrt\",\n",
    "    seed=42\n",
    ")\n",
    "rf_model = rf.fit(train_feat)\n",
    "\n",
    "# Predict + cache for metrics/report\n",
    "predictions = rf_model.transform(test_feat) \\\n",
    "    .select(\"DATE\", label_col, F.col(\"prediction\").alias(\"prediction\")) \\\n",
    "    .persist(StorageLevel(True, True, False, False, 1))\n",
    "_ = predictions.count()\n",
    "\n",
    "# Evaluate\n",
    "e_rmse = RegressionEvaluator(labelCol=label_col, predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "e_mae  = RegressionEvaluator(labelCol=label_col, predictionCol=\"prediction\", metricName=\"mae\")\n",
    "e_r2   = RegressionEvaluator(labelCol=label_col, predictionCol=\"prediction\", metricName=\"r2\")\n",
    "print(f\"[RF-local] RMSE={e_rmse.evaluate(predictions):.4f} | \"\n",
    "      f\"MAE={e_mae.evaluate(predictions):.4f} | R²={e_r2.evaluate(predictions):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[RF-local] RMSE=3.7224 | MAE=2.6887 | R²=0.9123"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Save the trained RF model\n",
    "rf_model.write().overwrite().save(\"models/rf_weather_model\")\n",
    "\n",
    "# Save predictions for reuse (optional but useful for analysis)\n",
    "predictions.write.mode(\"overwrite\").parquet(\"results/rf_predictions.parquet\")\n",
    "\n",
    "# Save imputer model too if you fitted one separately\n",
    "imputerModel.write().overwrite().save(\"models/rf_imputer_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the below one tomorrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RF-reload] RMSE=3.7224 | MAE=2.6887 | R²=0.9123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark import StorageLevel\n",
    "\n",
    "label_col = \"TMP_C\"\n",
    "feature_cols = [\n",
    "    \"LATITUDE\",\"LONGITUDE\",\"ELEVATION\",\n",
    "    \"WND_DIR_DEG\",\"WND_SPD_MS\",\n",
    "    \"CIG_HEIGHT_M\",\"VIS_DIST_M\",\n",
    "    \"DEW_C\",\"SLP_hPa\",\n",
    "    \"year\",\"month\",\"day\",\"hour\"\n",
    "]\n",
    "imp_cols = [c + \"_imp\" for c in feature_cols]\n",
    "\n",
    "# 0) Sanity: confirm the saved imputer expects the same inputs\n",
    "assert list(imputerModel.getInputCols()) == feature_cols, \\\n",
    "    f\"Imputer inputs {imputerModel.getInputCols()} don't match expected {feature_cols}\"\n",
    "\n",
    "# 1) Keep only what you need; ensure DATE is present for reporting\n",
    "req_cols = [\"DATE\", label_col] + feature_cols\n",
    "missing = [c for c in req_cols if c not in test_df.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"Missing columns in test_df: {missing}\")\n",
    "\n",
    "test_df = test_df.select(*req_cols)\n",
    "\n",
    "# 2) Transform with the *fitted* imputer, then assemble\n",
    "assembler = VectorAssembler(inputCols=imp_cols, outputCol=\"features\")\n",
    "test_feat = (\n",
    "    assembler\n",
    "    .transform(imputerModel.transform(test_df))\n",
    "    .select(\"DATE\", label_col, \"features\")\n",
    ")\n",
    "\n",
    "# 3) Predict + cache (serialized)\n",
    "SERIALIZED_STORAGE = StorageLevel(True, True, False, False, 1)\n",
    "predictions = (\n",
    "    rf_model.transform(test_feat)\n",
    "            .select(\"DATE\", F.col(label_col).alias(\"TMP_C\"), F.col(\"prediction\"))\n",
    "            .persist(SERIALIZED_STORAGE)\n",
    ")\n",
    "_ = predictions.count()  # materialize\n",
    "\n",
    "# 4) Evaluate & save\n",
    "e_rmse = RegressionEvaluator(labelCol=\"TMP_C\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "e_mae  = RegressionEvaluator(labelCol=\"TMP_C\", predictionCol=\"prediction\", metricName=\"mae\")\n",
    "e_r2   = RegressionEvaluator(labelCol=\"TMP_C\", predictionCol=\"prediction\", metricName=\"r2\")\n",
    "\n",
    "rmse = float(e_rmse.evaluate(predictions))\n",
    "mae  = float(e_mae.evaluate(predictions))\n",
    "r2   = float(e_r2.evaluate(predictions))\n",
    "print(f\"[RF-reload] RMSE={rmse:.4f} | MAE={mae:.4f} | R²={r2:.4f}\")\n",
    "\n",
    "# Save artifacts\n",
    "predictions.write.mode(\"overwrite\").parquet(\"results/rf_predictions_reload.parquet\")\n",
    "spark.createDataFrame([(rmse, mae, r2)], [\"rmse\",\"mae\",\"r2\"]) \\\n",
    "     .coalesce(1).write.mode(\"overwrite\").option(\"header\", True).csv(\"results/rf_metrics_reload_csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 89:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[saved] results/rf_metrics_csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "e_rmse = RegressionEvaluator(labelCol=\"TMP_C\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "e_mae  = RegressionEvaluator(labelCol=\"TMP_C\", predictionCol=\"prediction\", metricName=\"mae\")\n",
    "e_r2   = RegressionEvaluator(labelCol=\"TMP_C\", predictionCol=\"prediction\", metricName=\"r2\")\n",
    "\n",
    "rmse = float(e_rmse.evaluate(predictions))\n",
    "mae  = float(e_mae.evaluate(predictions))\n",
    "r2   = float(e_r2.evaluate(predictions))\n",
    "\n",
    "spark.createDataFrame([(rmse, mae, r2)], [\"rmse\",\"mae\",\"r2\"]) \\\n",
    "     .coalesce(1).write.mode(\"overwrite\").option(\"header\", True).csv(\"results/rf_metrics_csv\")\n",
    "print(f\"[saved] results/rf_metrics_csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 91:=============================================>       (428 + 10) / 500]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[saved] results/rf_feature_importance_csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "## Feature importance CSV\n",
    "# If rf_model is not in scope, reload it as above\n",
    "imp_vec = rf_model.featureImportances\n",
    "feat_imps = [(feature_cols[i], float(imp_vec[i])) for i in range(len(feature_cols))]\n",
    "spark.createDataFrame(feat_imps, [\"feature\",\"importance\"]) \\\n",
    "     .orderBy(F.desc(\"importance\")) \\\n",
    "     .coalesce(1).write.mode(\"overwrite\").option(\"header\", True).csv(\"results/rf_feature_importance_csv\")\n",
    "print(f\"[saved] results/rf_feature_importance_csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[saved] results/rf_rmse_by_hour_csv, results/rf_rmse_by_month_csv\n"
     ]
    }
   ],
   "source": [
    "## RMSE by hour and month\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "eval_df = (predictions\n",
    "    .withColumn(\"residual\", F.col(\"TMP_C\") - F.col(\"prediction\"))\n",
    "    .withColumn(\"sq_err\", F.pow(F.col(\"residual\"), 2))\n",
    "    .withColumn(\"hour\",  F.hour(\"DATE\"))\n",
    "    .withColumn(\"month\", F.month(\"DATE\"))\n",
    ")\n",
    "\n",
    "rmse_by_hour = eval_df.groupBy(\"hour\") \\\n",
    "    .agg(F.sqrt(F.avg(\"sq_err\")).alias(\"rmse\"),\n",
    "         F.avg(\"residual\").alias(\"bias\")) \\\n",
    "    .orderBy(\"hour\")\n",
    "rmse_by_hour.coalesce(1).write.mode(\"overwrite\").option(\"header\", True).csv(\"results/rf_rmse_by_hour_csv\")\n",
    "\n",
    "rmse_by_month = eval_df.groupBy(\"month\") \\\n",
    "    .agg(F.sqrt(F.avg(\"sq_err\")).alias(\"rmse\"),\n",
    "         F.avg(\"residual\").alias(\"bias\")) \\\n",
    "    .orderBy(\"month\")\n",
    "rmse_by_month.coalesce(1).write.mode(\"overwrite\").option(\"header\", True).csv(\"results/rf_rmse_by_month_csv\")\n",
    "\n",
    "print(f\"[saved] results/rf_rmse_by_hour_csv, results/rf_rmse_by_month_csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 110:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[saved] results/rf_predictions_csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "predictions.select(\"DATE\",\"TMP_C\",\"prediction\") \\\n",
    "    .coalesce(1).write.mode(\"overwrite\").option(\"header\", True).csv(\"results/rf_predictions_csv\")\n",
    "print(f\"[saved] results/rf_predictions_csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'results/rf_predictions_csv/part-*.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m os.makedirs(FIG_DIR, exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# --- 1. Predicted vs Actual Scatter ---\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresults/rf_predictions_csv/part-*.csv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m plt.figure(figsize=(\u001b[32m6\u001b[39m,\u001b[32m6\u001b[39m))\n\u001b[32m     12\u001b[39m plt.scatter(df[\u001b[33m\"\u001b[39m\u001b[33mTMP_C\u001b[39m\u001b[33m\"\u001b[39m], df[\u001b[33m\"\u001b[39m\u001b[33mprediction\u001b[39m\u001b[33m\"\u001b[39m], s=\u001b[32m5\u001b[39m, alpha=\u001b[32m0.5\u001b[39m, color=\u001b[33m\"\u001b[39m\u001b[33mteal\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Machine Learning/.conda/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Machine Learning/.conda/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Machine Learning/.conda/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Machine Learning/.conda/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Machine Learning/.conda/lib/python3.12/site-packages/pandas/io/common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'results/rf_predictions_csv/part-*.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# --- Paths ---\n",
    "FIG_DIR = \"results/figures\"\n",
    "os.makedirs(FIG_DIR, exist_ok=True)\n",
    "\n",
    "# --- 1. Predicted vs Actual Scatter ---\n",
    "df = pd.read_csv(\"results/rf_predictions_csv/part-*.csv\")\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.scatter(df[\"TMP_C\"], df[\"prediction\"], s=5, alpha=0.5, color=\"teal\")\n",
    "plt.plot([df[\"TMP_C\"].min(), df[\"TMP_C\"].max()],\n",
    "         [df[\"TMP_C\"].min(), df[\"TMP_C\"].max()],\n",
    "         'r--', lw=2, label=\"Perfect fit\")\n",
    "plt.xlabel(\"Actual Temperature (°C)\")\n",
    "plt.ylabel(\"Predicted Temperature (°C)\")\n",
    "plt.title(\"Random Forest – Predicted vs Actual TMP_C\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{FIG_DIR}/rf_pred_vs_actual.png\", dpi=300)\n",
    "plt.close()\n",
    "\n",
    "# --- 2. Residual Distribution Histogram ---\n",
    "df[\"residual\"] = df[\"TMP_C\"] - df[\"prediction\"]\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.hist(df[\"residual\"], bins=50, color=\"orange\", edgecolor=\"k\", alpha=0.7)\n",
    "plt.title(\"Residual Distribution (TMP_C - Predicted)\")\n",
    "plt.xlabel(\"Residual (°C)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{FIG_DIR}/rf_residual_hist.png\", dpi=300)\n",
    "plt.close()\n",
    "\n",
    "# --- 3. Feature Importance Bar Chart ---\n",
    "imp = pd.read_csv(\"results/rf_feature_importance_csv/part-*.csv\")\n",
    "imp = imp.sort_values(\"importance\", ascending=False)\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.barh(imp[\"feature\"], imp[\"importance\"], color=\"skyblue\")\n",
    "plt.gca().invert_yaxis()\n",
    "plt.title(\"Random Forest Feature Importance\")\n",
    "plt.xlabel(\"Importance Score\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{FIG_DIR}/rf_feature_importance.png\", dpi=300)\n",
    "plt.close()\n",
    "\n",
    "# --- 4. RMSE by Hour ---\n",
    "rmse_hour = pd.read_csv(\"results/rf_rmse_by_hour_csv/part-*.csv\")\n",
    "plt.figure(figsize=(7,4))\n",
    "plt.plot(rmse_hour[\"hour\"], rmse_hour[\"rmse\"], marker=\"o\", color=\"slateblue\")\n",
    "plt.title(\"RMSE by Hour of Day\")\n",
    "plt.xlabel(\"Hour\")\n",
    "plt.ylabel(\"RMSE (°C)\")\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{FIG_DIR}/rf_rmse_by_hour.png\", dpi=300)\n",
    "plt.close()\n",
    "\n",
    "# --- 5. RMSE by Month ---\n",
    "rmse_month = pd.read_csv(\"results/rf_rmse_by_month_csv/part-*.csv\")\n",
    "plt.figure(figsize=(7,4))\n",
    "plt.plot(rmse_month[\"month\"], rmse_month[\"rmse\"], marker=\"o\", color=\"seagreen\")\n",
    "plt.title(\"RMSE by Month\")\n",
    "plt.xlabel(\"Month\")\n",
    "plt.ylabel(\"RMSE (°C)\")\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{FIG_DIR}/rf_rmse_by_month.png\", dpi=300)\n",
    "plt.close()\n",
    "\n",
    "print(f\"✅ All RF figures saved in {FIG_DIR}/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
